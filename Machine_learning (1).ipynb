{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "aiNtjsmHHltf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud \n",
        "from cleantext import clean"
      ],
      "metadata": {
        "id": "HHUcundmMA9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data pipeline"
      ],
      "metadata": {
        "id": "SKCddgouHnkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "class Text_analysis():\n",
        "    polarity=0\n",
        "    subjectivity = 0\n",
        "    helpfulness = 1\n",
        "    tokens = []\n",
        "\n",
        "    def __init__(self,txt):\n",
        "        self.txt = txt\n",
        "    \n",
        "    def clean_emoji(self):\n",
        "        self.txt = clean(self.txt,no_emoji=True)\n",
        "        return\n",
        "\n",
        "    def clean_text(self):\n",
        "        self.txt = self.txt.lower()\n",
        "        self.txt = re.sub(r'@[A-Za-z0-9]+','',self.txt) #remove mentions\n",
        "        self.txt = re.sub(r'#','',self.txt) #remove hastags\n",
        "        self.txt = re.sub(r'RT[\\s]+','',self.txt) #removing RT\n",
        "        self.txt = re.sub(r'https?:\\/\\/\\s+','',self.txt) #removing the hyperlink\n",
        "        return\n",
        "    \n",
        "    def get_polarity(self):\n",
        "        self.polarity = TextBlob(self.txt).sentiment.polarity\n",
        "        return\n",
        "    \n",
        "    def get_subjectivity(self):\n",
        "        self.subjectivity = TextBlob(self.txt).sentiment.subjectivity \n",
        "        return\n",
        "    \n",
        "    def tokens(self):\n",
        "        self.tokens = word_tokenize(self.txt)\n",
        "        return\n",
        "    \n",
        "    def Stemming(self):\n",
        "        ps = PorterStemmer()\n",
        "        for i in range(len(self.tokens)):\n",
        "            self.tokens[i] = ps.stem(self.tokens[i])\n",
        "        self.txt = \" \".join(self.tokens)\n",
        "        return\n",
        "    \n",
        "    def Lemmatization(self):\n",
        "        lm = WordNetLemmatizer()\n",
        "        for i in range(len(self.tokens)):\n",
        "            self.tokens[i] = lm.lemmatize(self.tokens[i])\n",
        "        self.txt = \" \".join(self.tokens)\n",
        "        return\n",
        "    \n",
        "    def get_data():\n",
        "        return [txt, polarity, subjectivity, tokens, helpfulness]\n",
        "\n",
        "    def Pipeline(self):\n",
        "        self.clean_emoji()\n",
        "        self.clean_text()\n",
        "\n",
        "        self.tokens()\n",
        "        self.lemmatization()\n",
        "\n",
        "        self.get_polarity()\n",
        "        self.get_subjectivity()\n",
        "\n",
        "        return self.get_data()\n",
        "\n"
      ],
      "metadata": {
        "id": "NMnyQrfgG_9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary"
      ],
      "metadata": {
        "id": "bNHRkxjZHqZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary():\n",
        "    PAD_token = 0   # Used for padding short sentences\n",
        "    SOS_token = 1   # Start-of-sentence token\n",
        "    EOS_token = 2   # End-of-sentence token\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "        self.num_sentences = 0\n",
        "        self.longest_sentence = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            # First entry of word into vocabulary\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            # Word exists; increase word count\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "    def add_sentence(self, sentence):\n",
        "        sentence_len = 0\n",
        "        for word in sentence.split(' '):\n",
        "            sentence_len += 1\n",
        "            self.add_word(word)\n",
        "        if sentence_len > self.longest_sentence:\n",
        "            # This is the longest sentence\n",
        "            self.longest_sentence = sentence_len\n",
        "        # Count the number of sentences\n",
        "        self.num_sentences += 1\n",
        "\n",
        "    def to_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def to_index(self, word):\n",
        "        return self.word2index[word]"
      ],
      "metadata": {
        "id": "XHdfMV1PG_6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Rank"
      ],
      "metadata": {
        "id": "0wBdsrVFHyLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "class TextRank4Keyword():\n",
        "    \"\"\"Extract keywords from text\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.d = 0.85 # damping coefficient, usually is .85\n",
        "        self.min_diff = 1e-5 # convergence threshold\n",
        "        self.steps = 10 # iteration steps\n",
        "        self.node_weight = None # save keywords and its weight\n",
        "\n",
        "    \n",
        "    def set_stopwords(self, stopwords):  \n",
        "        \"\"\"Set stop words\"\"\"\n",
        "        for word in STOP_WORDS.union(set(stopwords)):\n",
        "            lexeme = nlp.vocab[word]\n",
        "            lexeme.is_stop = True\n",
        "    \n",
        "    def sentence_segment(self, doc, candidate_pos, lower):\n",
        "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
        "        sentences = []\n",
        "        for sent in doc.sents:\n",
        "            selected_words = []\n",
        "            for token in sent:\n",
        "                # Store words only with cadidate POS tag\n",
        "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
        "                    if lower is True:\n",
        "                        selected_words.append(token.text.lower())\n",
        "                    else:\n",
        "                        selected_words.append(token.text)\n",
        "            sentences.append(selected_words)\n",
        "        return sentences\n",
        "        \n",
        "    def get_vocab(self, sentences):\n",
        "        \"\"\"Get all tokens\"\"\"\n",
        "        vocab = OrderedDict()\n",
        "        i = 0\n",
        "        for sentence in sentences:\n",
        "            for word in sentence:\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = i\n",
        "                    i += 1\n",
        "        return vocab\n",
        "    \n",
        "    def get_token_pairs(self, window_size, sentences):\n",
        "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
        "        token_pairs = list()\n",
        "        for sentence in sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                for j in range(i+1, i+window_size):\n",
        "                    if j >= len(sentence):\n",
        "                        break\n",
        "                    pair = (word, sentence[j])\n",
        "                    if pair not in token_pairs:\n",
        "                        token_pairs.append(pair)\n",
        "        return token_pairs\n",
        "    def symmetrize(self, a):\n",
        "        return a + a.T - np.diag(a.diagonal())\n",
        "    \n",
        "    def get_matrix(self, vocab, token_pairs):\n",
        "        \"\"\"Get normalized matrix\"\"\"\n",
        "        # Build matrix\n",
        "        vocab_size = len(vocab)\n",
        "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
        "        for word1, word2 in token_pairs:\n",
        "            i, j = vocab[word1], vocab[word2]\n",
        "            g[i][j] = 1\n",
        "            \n",
        "        # Get Symmeric matrix\n",
        "        g = self.symmetrize(g)\n",
        "        \n",
        "        # Normalize matrix by column\n",
        "        norm = np.sum(g, axis=0)\n",
        "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
        "        \n",
        "        return g_norm\n",
        "\n",
        "    \n",
        "    def get_keywords(self, number=10):\n",
        "        \"\"\"Print top number keywords\"\"\"\n",
        "        ls = []\n",
        "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
        "        for i, (key, value) in enumerate(node_weight.items()):\n",
        "            ls.append(key + ' - ' + str(value))\n",
        "            if i > number:\n",
        "                break\n",
        "        return ls\n",
        "    def analyze(self, text, \n",
        "                candidate_pos=['NOUN', 'PROPN'], \n",
        "                window_size=4, lower=False, stopwords=list()):\n",
        "        \"\"\"Main function to analyze text\"\"\"\n",
        "        \n",
        "        # Set stop words\n",
        "        self.set_stopwords(stopwords)\n",
        "        \n",
        "        # Pare text by spaCy\n",
        "        doc = nlp(text)\n",
        "        \n",
        "        # Filter sentences\n",
        "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
        "        \n",
        "        # Build vocabulary\n",
        "        vocab = self.get_vocab(sentences)\n",
        "        \n",
        "        # Get token_pairs from windows\n",
        "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
        "        \n",
        "        # Get normalized matrix\n",
        "        g = self.get_matrix(vocab, token_pairs)\n",
        "        pr = np.array([1] * len(vocab))\n",
        "        \n",
        "        # Iteration\n",
        "        previous_pr = 0\n",
        "        for epoch in range(self.steps):\n",
        "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
        "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
        "                break\n",
        "            else:\n",
        "                previous_pr = sum(pr)\n",
        "\n",
        "        # Get weight for each node\n",
        "        node_weight = dict()\n",
        "        for word, index in vocab.items():\n",
        "            node_weight[word] = pr[index]\n",
        "        \n",
        "        self.node_weight = node_weight"
      ],
      "metadata": {
        "id": "yq0caEgwG_3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def Cosine_Similarity():\n",
        "    Tfidf_vect = TfidfVectorizer()\n",
        "    vector_matrix = Tfidf_vect.fit_transform(data)\n",
        "\n",
        "    return cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
        "    \n"
      ],
      "metadata": {
        "id": "1Yn4HrWYSqRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trending Score"
      ],
      "metadata": {
        "id": "hlSSXGz7A4Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for i in range(len(mobile)):\n",
        "\n",
        "    view_count = int((viewframe.loc[viewframe['Keyword']==mobile[i],['view_count']].sum())/(len(viewframe['Mobile']==mobile[i])))\n",
        "\n",
        "    like_count = int((viewframe.loc[viewframe['Keyword']==mobile[i],['like_count']].sum())/(len(viewframe['Mobile']==mobile[i])))\n",
        "\n",
        "    comment_count = int((viewframe.loc[viewframe['Keyword']==mobile[i],['comment_count']].sum())/(len(viewframe['Mobile']==mobile[i])))\n",
        "\n",
        "    pol = float((commframe.loc[commframe['Keyword']==mobile[i],['Polarity']].sum())/(len(commframe['Mobile']==mobile[i])))\n",
        "\n",
        "    subj = float((commframe.loc[commframe['Keyword']==mobile[i],['Subjectivity']].sum())/(len(commframe['Mobile']==mobile[i])))\n",
        "\n",
        "    sent = float((commframe.loc[commframe['Keyword']==mobile[i],['Sentiment']].sum())/(len(commframe['Mobile']==mobile[i])))\n",
        "\n",
        "    data.append([mobile[i],view_count,like_count,comment_count,pol,subj,sent])"
      ],
      "metadata": {
        "id": "8F7Da4q4A-iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Trend_score'] = df['view_count']*df['like_count']*df['Avg_sentiment']"
      ],
      "metadata": {
        "id": "nkPak0scBAsh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}